---
sidebar_position: 1
---

# 逻辑回归 (Logistic Regression)

逻辑回归是一种用于二分类问题的线性分类算法，是机器学习中最基础也是最重要的算法之一。

## 算法原理

### 从线性回归到逻辑回归的演进

让我们从一个简单的问题开始：如何预测一个学生是否能通过考试？如果我们用线性回归，可能会得到这样的结果：$y = 0.3x_1 + 0.5x_2 + 0.2$，其中$x_1$是学习时间，$x_2$是作业完成度。

但是这里有个问题：线性回归的输出可以是任意实数，比如-2.5或者15.3，这显然不能直接表示"通过考试的概率"。概率必须在0到1之间，而且应该有一个合理的解释。

这就是逻辑回归要解决的问题。它的核心思想很简单：**既然线性回归的输出范围不合适，我们就用一个函数把它"压缩"到0到1之间**。这个函数就是sigmoid函数。

### 为什么选择Sigmoid函数？

想象一下，我们希望有一个函数，当线性组合$z = \theta^T x$很大时，输出接近1（表示高概率）；当$z$很小时，输出接近0（表示低概率）；当$z = 0$时，输出正好是0.5（表示不确定）。

Sigmoid函数完美地满足了这个要求：
$$h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$$

这个函数有几个很好的性质：
- 当$\theta^T x \to +\infty$时，$h_\theta(x) \to 1$
- 当$\theta^T x \to -\infty$时，$h_\theta(x) \to 0$  
- 当$\theta^T x = 0$时，$h_\theta(x) = 0.5$

### 概率建模的数学基础

现在我们已经有了一个可以输出概率的函数，但如何确保这个概率是有意义的呢？这里需要引入一些概率论的知识。

我们假设每个样本的标签$y$服从伯努利分布，也就是说：
- 如果真实标签是1（正类），那么$y$取1的概率是$p$，取0的概率是$1-p$
- 如果真实标签是0（负类），那么$y$取0的概率是$1-p$，取1的概率是$p$

用数学表达就是：
$$P(y|x) = p^y(1-p)^{1-y}$$

其中$p = h_\theta(x)$就是我们模型预测的概率。

### 如何找到最优参数？

现在的问题是：如何找到最好的参数$\theta$，使得我们的模型预测最准确？

这里我们使用最大似然估计的思想。简单来说，我们希望找到参数$\theta$，使得在给定这些参数的情况下，观察到当前这些训练数据的概率最大。

对于所有训练样本，总的似然函数是：
$$L(\theta) = \prod_{i=1}^{m}P(y^{(i)}|x^{(i)}) = \prod_{i=1}^{m}h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$$

为了计算方便，我们通常取对数，得到对数似然函数：
$$\ell(\theta) = \sum_{i=1}^{m}[y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$$

我们的目标就是最大化这个对数似然函数。

### 数学原理详解

#### Sigmoid函数的性质

**函数定义**：
$$h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} = \frac{e^{\theta^T x}}{1 + e^{\theta^T x}}$$

**重要性质**：
- **单调性**：严格单调递增函数
- **有界性**：输出范围(0,1)，永远不等于0或1
- **对称性**：关于点(0, 0.5)中心对称
- **导数性质**：$h'(z) = h(z)(1-h(z))$

**为什么选择Sigmoid函数**：
- **概率解释**：输出可以解释为概率
- **可微性**：处处可微，便于梯度下降
- **饱和性**：在极值处梯度小，防止过拟合
- **对称性**：关于0.5对称，符合二分类特性

#### 损失函数的推导

**对数似然函数**：
$$\ell(\theta) = \log L(\theta) = \sum_{i=1}^{m}[y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$$

**负对数似然损失**：
$$J(\theta) = -\frac{1}{m}\ell(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$$

**损失函数的直观理解**：
- 当$y=1$时，$J = -\log h_\theta(x)$，预测概率越高损失越小
- 当$y=0$时，$J = -\log(1-h_\theta(x))$，预测概率越低损失越小
- 完美预测时损失为0，完全错误时损失趋向无穷大

#### 梯度推导过程

**Sigmoid函数导数**：
$$\frac{d}{dz}h_\theta(x) = h_\theta(x)(1-h_\theta(x))$$

**损失函数对参数的梯度**：
$$\frac{\partial J}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$

**梯度推导步骤**：
1. 计算损失函数对$h_\theta(x)$的偏导数
2. 计算$h_\theta(x)$对$\theta_j$的偏导数
3. 应用链式法则得到最终梯度

**梯度的直观含义**：
- 梯度大小与预测误差$(h_\theta(x) - y)$成正比
- 梯度方向与特征$x_j$成正比
- 当预测正确时梯度为0，预测错误时梯度不为0

## 算法关键部分

### 参数优化

逻辑回归使用**梯度下降**算法来优化参数：

1. **梯度计算**：计算损失函数对每个参数的偏导数
   $$\frac{\partial J}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$

2. **参数更新**：沿着梯度反方向更新参数
   $$\theta_j := \theta_j - \alpha \frac{\partial J}{\partial \theta_j}$$

3. **收敛条件**：当梯度接近零或达到最大迭代次数时停止

### 正则化技术

**L1正则化（Lasso）**：
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))] + \lambda\sum_{j=1}^{n}|\theta_j|$$

**L2正则化（Ridge）**：
$$J(\theta) = -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))] + \lambda\sum_{j=1}^{n}\theta_j^2$$

### 多分类扩展

- **One-vs-Rest (OvR)**：为每个类别训练一个二分类器
- **One-vs-One (OvO)**：为每对类别训练一个二分类器
- **Softmax回归**：直接处理多分类问题
  $$P(y=k|x) = \frac{e^{\theta_k^T x}}{\sum_{j=1}^{K}e^{\theta_j^T x}}$$

## 面试常见问题

### 1. 为什么逻辑回归使用对数似然损失而不是均方误差？

这是一个经典的面试问题。主要原因包括：

**概率解释**：逻辑回归的目标是输出概率，对数似然损失直接对应概率的最大似然估计，而均方误差没有概率意义。

**梯度性质**：对数似然损失的梯度在预测错误时更大，学习效果更好。而均方误差在sigmoid函数饱和区域梯度很小，学习困难。

**数值稳定性**：对数似然损失避免了sigmoid函数在极值处的梯度消失问题，而均方误差会导致梯度消失。

**理论依据**：对数似然损失符合二项分布的概率模型，而均方误差假设的是高斯分布。

### 2. 如何处理逻辑回归中的样本不平衡问题？

**问题表现**：当正负样本比例严重失衡时，逻辑回归会偏向多数类，导致少数类识别效果差。

**解决方案**：
- **类别权重**：给少数类样本更高的权重，如`class_weight='balanced'`
- **阈值调整**：将分类阈值从0.5调整到更合适的值，如0.3或0.7
- **SMOTE采样**：生成少数类样本的合成样本，平衡数据集
- **代价敏感学习**：在损失函数中给不同类别分配不同的权重

**评估指标**：使用精确率、召回率、F1分数等指标，而不是简单的准确率。

### 4. L1和L2正则化的区别？

**L1正则化（Lasso）**：
- **特征选择**：能够将不重要的特征权重压缩为0
- **稀疏解**：产生稀疏的权重向量
- **适用场景**：特征数量多，需要特征选择时

**L2正则化（Ridge）**：
- **权重衰减**：将权重向零收缩，但不完全为0
- **平滑解**：产生平滑的权重向量
- **适用场景**：特征间存在多重共线性时

### 5. 如何处理多分类问题？

**One-vs-Rest (OvR)**：
- 为每个类别训练一个二分类器
- 选择概率最高的类别作为预测结果
- 计算复杂度：O(K)，其中K是类别数

**One-vs-One (OvO)**：
- 为每对类别训练一个二分类器
- 通过投票决定最终类别
- 计算复杂度：O(K²)

**Softmax回归**：
- 直接扩展逻辑回归到多分类
- 输出所有类别的概率分布
- 计算复杂度：O(K)

### 6. 逻辑回归的收敛性如何保证？

- **凸优化**：逻辑回归的损失函数是凸函数，保证全局最优解
- **梯度下降**：在合适的学习率下，梯度下降算法保证收敛
- **学习率选择**：过大会震荡，过小会收敛慢
- **特征标准化**：有助于梯度下降的稳定收敛

### 7. 实际应用中的调参技巧？

- **正则化参数λ**：通过交叉验证选择，平衡偏差和方差
- **学习率α**：通常从0.01开始，根据收敛情况调整
- **最大迭代次数**：设置足够大，通过早停避免过拟合
- **特征工程**：标准化、特征选择、多项式特征等
